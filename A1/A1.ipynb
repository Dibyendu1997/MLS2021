{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q)prove that under Gaussian assumption linear regression amounts to least square\n",
    "When faced with a regression problem, why might linear regression, and\n",
    "specifically why might the least-squares cost function J, be a reasonable\n",
    "choice? In this section, we will give a set of probabilistic assumptions, under\n",
    "which least-squares regression is derived as a very natural algorithm.\n",
    "Let us assume that the target variables and the inputs are related via the\n",
    "equation$$\\\\y^{(i)}=\\theta^{T} x^{(i)} \\ +\\ \\epsilon^{(i)}$$\\where $\\epsilon^{(i)}$ is an error term that captures either unmodeled effects (such as\n",
    "if there are some features very pertinent to predicting housing price, but\n",
    "that weâ€™d left out of the regression), or random noise. Let us further assume\n",
    "that the  $\\epsilon^{(i)}$ are distributed IID (independently and identically distributed)\n",
    "according to a Gaussian distribution (also called a Normal distribution) with mean zero and some variance $\\sigma$. We can write this assumption as $$\\epsilon^{(i)}\\ \\sim \\ \\mathcal{N}(0,\\sigma^2)$$\n",
    " I.e., the density of $\\epsilon^{(i)}$ is given by\\$$p(\\epsilon^{(i)})\\ =\\ \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(\\epsilon^{i})^2}{2\\sigma^{2}}\\right)$$This implies that $$p(y^{i}|x^{i};\\theta)\\ =\\ \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y^{(i)}-\\theta^{T} x^{(i)})^2}{2\\sigma^{2}}\\right)2$$ The notation $p(y^{i}|x^{i};\\theta)$  indicates that this is the  distribution of $y^{(i)}$ given $x^{(i)}$ and parameterized by $\\theta$. Note that we should not condition on $\\theta$ $(p(y^{i}|x^{i};\\theta)$  since $\\theta$ is not a random variable. We can also write the distribution of $y^{(i)}$ as as $y^{(i)}$|$x^{(i)}$; $\\theta\\ \\sim\\ \\mathcal{N}(\\theta^{T} x^{(i)},\\sigma^2)$ The probability of the data is given by\n",
    " $p(\\vec{y}|X;\\theta)$. This quantity is typically viewed a function of $\\vec{y}$ (and perhaps $X$),\n",
    " for a fixed value of $\\theta$. When we wish to explicitly view this as a function of $\\theta$, we will instead call it the $\\textbf{likelihood}$  function$$\\textbf{L}(\\theta)\\ =\\ \\textbf{L}(\\theta,\\textbf{X},\\vec{y})\\ = \\ p(\\vec{y}|X;\\theta)$$ $$= \\prod_{i=1}^{m}p(y^{i}|x^{i};\\theta)=\\ \\prod_{i=1}^{m}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y^{(i)}-\\theta^{T} x^{(i)})^2}{2\\sigma^{2}}\\right)$$\n",
    " Instead of maximizing $\\textbf{L}(\\theta)$, we can also maximize any strictly increasing\n",
    " function of \\textbf{L($\\theta$)}. In particular, the derivations will be a bit simpler if we\n",
    " instead maximize the $\\textbf{log likelihood}$\n",
    "  $$\\ell(\\theta)=\\log\\textbf{L}(\\theta)=\\  \\log\\prod_{i=1}^{m}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y^{(i)}-\\theta^{T} x^{(i)})^2}{2\\sigma^{2}}\\right)$$ $$=\\sum_{i=1}^{m}\\log \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y^{(i)}-\\theta^{T} x^{(i)})^2}{2\\sigma^{2}}\\right)$$\n",
    "  $$=m\\log\\frac{1}{\\sqrt{2\\pi}\\sigma}-\\frac{1}{\\sigma^2}*\\frac{1}{2}\\sum_{i=1}^{m}(y^{(i)}-\\theta^{T} x^{(i)})^2$$ \\\\Hence, maximizing  gives the same answer as minimizing $\\ell(\\theta)$ $$\\frac{1}{2}\\sum_{i=1}^{m}(y^{(i)}-\\theta^{T} x^{(i)})^2$$\\\\which we recognize to be $\\textbf{J}(\\theta)$, our original least-squares cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
